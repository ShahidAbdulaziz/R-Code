---
title: "Final"
author: "Shahid Abdulaziz"
date: "8/22/2020"
output: word_document
---

# Problem 1

## Data preparation 

**A.** Load the dataset insurance.csvPreview the document into memory.

```{r , echo = TRUE, warning = FALSE}
library(readr)
library(caret)
library(MASS)
library(tree)
library(randomForest)
library(e1071)
library(factoextra)
library(cluster)
library(neuralnet)
library(kableExtra)


insurance <- read_csv("C:/Grad School/DSCI 512/Week 8/Final/insurance.csv") # reading in the data

insurance$sex <- as.factor(insurance$sex) #changing the variable class type
insurance$region <- as.factor(insurance$region)
insurance$smoker <- as.factor(insurance$smoker)

```

**B.** In the data frame, transform the variable charges by setting insurance$charges = log(insurance$charges). Do not transform it outside of the data frame.

```{r , echo = TRUE, warning = FALSE}

insurance$charges <-  log(insurance$charges) # transforming data
```

**C.** Using the data set from 1.b, use the model.matrix() function to create another data set that uses dummy variables in place of categorical variables. Verify that the first column only has ones (1) as values, and then discard the column only after verifying it has only ones as values.

```{r , echo = TRUE, warning = FALSE}

insurance_dummy <- model.matrix(~.,insurance) # creating a dummy data set

dim(table(insurance_dummy[,1])) == 1 # verifying first column only contains the value 1

insurance_dummy <- insurance_dummy[, -(1)] #getting rid of the intercept column


```

**D.** Use the sample() function with set.seed equal to 1 to generate row indexes for your training and tests sets, with 2/3 of the row indexes for your training set and 1/3 for your test set. Do not use any method other than the sample() function for splitting your data.

```{r , echo = TRUE, warning = FALSE}
set.seed(1)


RowIndexes <- sample(seq_len(nrow(insurance_dummy)), size = ((2/3)* nrow(insurance_dummy))) #Creating row indexes



```

**E.** Create a training and test data set from the data set created in 1.b using the training and test row indexes created in 1.d. Unless otherwise stated, only use the training and test data sets created in this step.

```{r , echo = TRUE, warning = FALSE}
train_1b <- insurance[RowIndexes, ] #Creating test and train data set for 1C
test_1b  <- insurance[-RowIndexes, ]
```

**F.** Create a training and test data set from data set created in 1.c using the training and test row indexes created in 1.d

```{r , echo = TRUE, warning = FALSE}
train_1C<- insurance_dummy[RowIndexes, ] #Creating test and train data set for 1B
test_1C <- insurance_dummy[-RowIndexes, ]
```

#Problem 2

## Build a multiple linear regression model

**A.** Perform multiple linear regression with charges as the response and the predictors are age, sex, bmi, children, smoker, and region. Print out the results using the summary() function. Use the training data set created in step 1.e to train your model.

```{r , echo = TRUE, warning = FALSE}
Multi_LinearMod_1B <- lm(charges ~ age + sex + bmi + children + smoker + region,  data = train_1b) #Creating mutli linear model

summary(Multi_LinearMod_1B)
```

**B.** Is there a relationship between the predictors and the response?

Yes. there is a relationship because the p-value for the model is p-value: < 2.2e-16

**C.** Does sex have a statistically significant relationship to the response?

At a 5% confidence level, sex does have a significant relationship to the reponse at a p-value of 0.017467.

**D.** Perform best subset selection using the stepAIC() function from the MASS library, choose best model based on AIC. For the "direction" parameter in the stepAIC() method, set direciton="backward"

```{r , echo = TRUE, warning = FALSE}
Multi_LinearMod_1B.backwards <- stepAIC(Multi_LinearMod_1B ,IC= "AIC", direction = "backward") #perfomring stepAIC()

Multi_LinearMod_1B.backwards #Best Model


```

**E.** Compute the test error of the best model in #3d based on AIC using LOOCV using trainControl() and train() from the caret library. Report the MSE by squaring the reported RMSE.

```{r , echo = TRUE, warning = FALSE}
train_controlLOOCV <- trainControl(method = "LOOCV") # Setting my paramter for control

BestModel_1B_LOOC  <- train(charges ~ age + sex + bmi + children + smoker + region, data =  train_1b, trainControl=train_controlLOOCV) # Using the best model with LOOCV paramter

mean((test_1b$charges - predict(BestModel_1B_LOOC ,  test_1b)) ^ 2) #MSE
```

**F.** Calculate the test error of the best model in #3d based on AIC using 10-fold Cross-Validation. Use train and trainControl from the caret library. Refer to model selected in #3d based on AIC. Report the MSE.

```{r , echo = TRUE, warning = FALSE}
train_control10fold <- trainControl(method = "CV", number = 10) # Setting my parameter for control

BestModel_1B_10fold <- train(charges ~ age + sex + bmi + children + smoker + region, data =  train_1b, trainControl=train_control10fold) # Using the best model with tenfold parameter

mean((test_1b$charges - predict(BestModel_1B_10fold  ,  test_1b)) ^ 2) #MSE
```

**G.** Calculate and report the test MSE using the best model from 2.d and test data set created in step 1.e.

```{r , echo = TRUE, warning = FALSE}
BestModel_1B <- train(charges ~ age + sex + bmi + children + smoker + region, data =  train_1b) # Using the best model 
mean((test_1b$charges - predict(BestModel_1B ,  test_1b)) ^ 2) #MSE
```

**H.** Compare the test MSE calculated in step 2.f using 10-fold cross-validation with the test MSE calculated in step 2.g. How similar are they?

```{r , echo = TRUE, warning = FALSE}

mean((test_1b$charges - predict(BestModel_1B_10fold ,  test_1b)) ^ 2) #MSE
```

Model using tenfold MSE is 0.1561342 and the model not having any train parameter MSE is 0.156656. They are both really close in error to one another. 

#problem 3

## Build a regression tree model

**A.** 

Build a regression tree model using function tree(), where charges is the response and the predictors are age, sex, bmi, children, smoker, and region.

```{r , echo = TRUE, warning = FALSE}
Tree_model_1b <- tree(charges ~ age + sex + bmi + children + smoker + region, data =  train_1b) #Creating tree model
```

**B.** 

Find the optimal tree by using cross-validation and display the results in a graphic. Report the best size.


```{r , echo = TRUE, warning = FALSE}
Best_Tree_Model <- cv.tree(Tree_model_1b)  #Finding best tree with tenfold
plot(Best_Tree_Model$size,Best_Tree_Model$dev, ttpe = 'b')  #plotting tree
```
4 is the bets size.

**C.** 

Justify  the number you picked for the optimal tree with regard to the principle of variance-bias trade-off.

The best size I am picking is 4. It has close to the same error has 5 wiht being slightly higher but it is also a much simplier model than size 5.

**D.** 

Prune the tree using the optinal size found in 3.b

```{r , echo = TRUE, warning = FALSE}
prune_Best_Tree_Model  <- prune.tree(Tree_model_1b , best = 4) #pruning the tree which I thought was the best
```

**E.** 

Plot the best tree model and give labels.

```{r , echo = TRUE, warning = FALSE}
plot(prune_Best_Tree_Model ) #plotting the best tree
text(prune_Best_Tree_Model , pretty = 0)
```

**F.** 

Calculate the test MSE for the best model.

```{r , echo = TRUE, warning = FALSE}
Predict_Prune_Best_Tree_Model<- predict(prune_Best_Tree_Model, data = test_1b) #predicting reuslts with mt testing dataset

mean((Predict_Prune_Best_Tree_Model- test_1b$charges )^2) #Calculating MSE for my best tree model
```
# Problem 4

## Build a random forest model

**A.** Build a random forest model using function randomForest(), where charges is the response and the predictors are age, sex, bmi, children, smoker, and region.
```{r , echo = TRUE, warning = FALSE}

RandomForestModel_1b <- randomForest(charges ~ age + sex + bmi + children + smoker + region, data =  train_1b, importance= TRUE) # Creating  random forest model
```

**B.** Compute the test error using the test data set.

```{r , echo = TRUE, warning = FALSE}

Random_Forest_Model_Prediction <- predict(RandomForestModel_1b, newdata= test_1b) #Creating a random forest model

mean((Random_Forest_Model_Prediction- test_1b$charges  )^2) #Calculating MSE
```

**C.** Extract variable importance measure using the importance() function.

```{r , echo = TRUE, warning = FALSE}
importance(RandomForestModel_1b) #Extracting variable importance
```

**D.** Plot the variable importance using the function, varImpPlot(). Which are the top 3 important predictors in this model?

```{r , echo = TRUE, warning = FALSE}
varImpPlot(RandomForestModel_1b) #Plotting my variable importance
```
# problem 5

## Build a support vector machine model

**A.** The response is charges and the predictors are age, sex, bmi, children, smoker, and region. Please use the svm() function with radial kernel and gamma=5 and cost = 50.

```{r , echo = TRUE, warning = FALSE}
SVM_Model <- svm(charges ~ age + sex + bmi + children + smoker + region, data =  train_1b,  kernal = "radial", gamma = 5, cost = 50) #making a svm model
```

**B.** Perform a grid search to find the best model with potential cost: 1, 10, 50, 100 and potential gamma: 1,3 and 5 and potential kernel: "linear","radial" and "sigmoid". And use the training set created in step 1.e.

```{r , echo = TRUE, warning = FALSE}
Best_SVM_Model <- tune(svm,charges ~ age + sex + bmi + children + smoker + region, data =  train_1b,  kernal = c("radial", "linear","sigmoid"), gamma = c(1,3,5), ranges = list(cost = c(1,10,50,100))) #Selecting the best SVM model
```

**C.** Print out the model results. What are the best model parameters?

```{r , echo = TRUE, warning = FALSE}
summary(Best_SVM_Model)
```

Cost of 1 is the best paramter

**D.** Forecast charges using the test dataset and the best model found in c).

```{r , echo = TRUE, warning = FALSE}
SVM_Predictions <- predict(Best_SVM_Model$best.model, test_1b) #Forecasting model
```

**E.** Compute the MSE (Mean Squared Error) on the test data.

```{r , echo = TRUE, warning = FALSE}
mean((SVM_Predictions- test_1b$charges  )^2) #Calculating MSE
```
# problem 6

## Peform the k-means cluster analysis 

**A.** Remove the sex, smoker, and region, since they are not numerical values.

```{r , echo = TRUE, warning = FALSE}
Drop <- c("sex", "smoker", "region")

insurance_dropped  <- insurance[,!names(insurance) %in% Drop]
```

**B.** Determine the optimal number of clusters. Justify your answer. It may take longer running time since it uses a large dataset.

```{r , echo = TRUE, warning = FALSE}
OptKmean <- fviz_nbclust(insurance_dropped , kmeans, method = "gap_stat") #Finding the optimal number of clusters


OptKmean
```

**C.** Perform k-means clustering using the 3 clusters.

```{r , echo = TRUE, warning = FALSE}
Kmeans_Model1 <- kmeans(insurance_dropped,3 , nstart = 25) #Creating kmeans model with 3 clusters like the directions said

Kmeans_Model1


```

**D.** Visualize the clusters in different colors.

```{r , echo = TRUE, warning = FALSE}
fviz_cluster(Kmeans_Model1,insurance_dropped) #Creating a visual for the results of my kmeans model
```
# problem 7

## Build a neural network model

**A.** Remove the sex, smoker, and region, since they are not numerical values.

```{r , echo = TRUE, warning = FALSE}
Drop <- c("sex", "smoker", "region") 

insurance_dropped  <- insurance[,!names(insurance) %in% Drop] #dropping columns
```

**B.** Standardize the inputs using the scale() function.

```{r , echo = TRUE, warning = FALSE}
insurance_scaled <- scale(insurance_dropped) #scaling my data
```

**C.** Convert the standardized inputs to a data frame using the as.data.frame() function.

```{r , echo = TRUE, warning = FALSE}
df.insurance_scaled <- as.data.frame(insurance_scaled ) #converting to df
```

**D.** Split the dataset into a training set containing 80% of the original data and the test set containing the remaining 20%.

```{r , echo = TRUE, warning = FALSE}
df.insurance_scaled_indx <- createDataPartition(df.insurance_scaled$charges , p = .8, list = FALSE) #Partitioning my data

Train_scaled <- df.insurance_scaled [df.insurance_scaled_indx ,] #Creating a train data set
Test_scaled  <- df.insurance_scaled [-df.insurance_scaled_indx ,] #Creating a test data set

```

**E.** The response is charges and the predictors are age, bmi, and children. Please use 1 hidden layer with 1 neuron.

```{r , echo = TRUE, warning = FALSE}
First_NeuralNet_Model <- neuralnet(charges~age + bmi + children, data = Train_scaled , hidden =c(1), linear.output = F) #Creating a neural net model
```

**F.** Plot the neural networks.

```{r , echo = TRUE, warning = FALSE}
plot(First_NeuralNet_Model) #Making a plot of the model
```

**G.** Forecast the charges in the test dataset.

```{r , echo = TRUE, warning = FALSE}
Neuralpredictions <- predict(First_NeuralNet_Model, Test_scaled) #Predicting with test data set
```

**H.** Get the observed charges of the test dataset.

```{r , echo = TRUE, warning = FALSE}
Test_scaled$charges #Getting the observed wine charges
```

**I.** Compute test error (MSE).

```{r , echo = TRUE, warning = FALSE}
mean((Test_scaled$charges  -Neuralpredictions )^2) #Calculating MSE
```
#Problem 8

## Putting it all together

**A.** For predicting insurance charges, your supervisor asks you to choose the best model among the multiple regression, regression tree, random forest, support vector machine, and neural network models. Compare the test MSEs of the models generated in steps 2.g, 3.f, 4.b, 5.e, and 7.d. Display the names for these types of these models, using these labels: Multiple Linear Regression, Regression Tree, Random Forest, Support Vector Machine, and Neural Network and their corresponding test MSEs in a data.frame. Label the column in your data frame with the labels as Model.Type, and label the column with the test MSEs as Test.MSE and round the data in this column to 4 decimal places. Present the formatted data to your supervisor and recommend which model is best and why.

```{r , echo = TRUE, warning = FALSE}

#Storing the MSE from the models requested into a variable 

BestModel_1B_MSE <- mean((test_1b$charges - predict(BestModel_1B ,  test_1b)) ^ 2) 

prune_Best_Tree_Model_MSE <- mean((test_1b$charges - predict(prune_Best_Tree_Model ,  test_1b)) ^ 2) 


RandomForestModel_1b_MSE <- mean((test_1b$charges - predict(RandomForestModel_1b,  test_1b)) ^ 2) 

SVM_Predictions_MSE  <- mean((SVM_Predictions- test_1b$charges  )^2)

First_NeuralNet_Model <- mean((Test_scaled$charges  -Neuralpredictions )^2)

# Creating the row names for my df 
Model.Type <- c("Multiple regression","Regression Tree","random forest","Support Vector Machine","neural network models")
# Creating the column for my model MSE
Test.MSE <- c(BestModel_1B_MSE, prune_Best_Tree_Model_MSE, RandomForestModel_1b_MSE,SVM_Predictions_MSE,First_NeuralNet_Model)

#Binding the rownames and MSE together into a df
AllModelDF <- as.data.frame(cbind(Model.Type,Test.MSE))

#Ordering my DF by MSE value
AllModelDF  <- AllModelDF[order(AllModelDF$Test.MSE, decreasing = FALSE), ]

AllModelDF$Test.MSE <-  as.numeric(AllModelDF$Test.MSE)
kable(AllModelDF, digits = 4, caption  = "Models Test MSE")
```

The best model I would pick is the Multiple regression model because it has the lowest MSE. Additionally, this model is also one of the simplest models in the list that is easier to explain and interpret. Having interpretable  makes it easier to explain to people who are business leaders. 

**B.** Another supervisor from the sales department has requested your help to create a predictive model that his sales representatives can use to explain to clients what the potential costs could be for different kinds of customers, and they need an easy and visual way of explaining it. What model would you recommend, and what are the benefits and disadvantages of your recommended model compared to other models?

Like above reasons to *.A, I would recommend a model that is not a black box model like  a multi linear model or a decision tree mode. These models are easily explain and can be accompanied by strong visuals to support its findings. However, these models can often have less predictive power than the block box methods below that can lead to a higher error. This trade off is something you have to consider when you are looking to be able to explain and visual a model for clients who might not be as familiar or trusting of black box methods. After some relationship and trust has been established with the client, you can then start to move to more advanced black box methods for them. 

**C.** The supervisor from the sales department likes your regression tree model. But she says that the salespeople say the numbers in it are way too low and suggests that maybe the numbers on the leaf nodes predicting charges are log transformations of the actual charges. You realize that in step 1.b of this project that you had indeed transformed charges using the log function. And now you realize that you need to reverse the transformation in your final output. The solution you have is to reverse the log transformation of the variables in the regression tree model you created and redisplay the result.
Follow these steps:
Copy your pruned tree model to a new variable.
In your new variable, find the data.frame named "frame" and reverse the log transformation on the data.frame column yval using the exp() function. (If the copy of your pruned tree model is named copy_of_my_pruned_tree, then the data frame is accessed as copy_of_my_pruned_tree$frame, and it works just like a normal data frame.).
After you reverse the log transform on the yval column, then replot the tree with labels.

```{r , echo = TRUE, warning = FALSE}
Prune_Model_Unlog  <- prune_Best_Tree_Model
Prune_Model_Unlog$y <- exp(Prune_Model_Unlog$y) #reverse logging value

plot(Prune_Model_Unlog ) #plotting the best tree
text(Prune_Model_Unlog , pretty = 0)
```
